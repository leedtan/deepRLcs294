3.6.0 |Anaconda custom (64-bit)| (default, Dec 23 2016, 12:22:00) 
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]
loading and building expert policy
obs (1, 376) (1, 376)
loaded and built
iter 0
iter 1
100/1000
iter 2
100/1000
returns [424.867445639972, 828.17343916878019, 1079.6825250149802]
mean return 777.574469941
std of return 269.710818976
learning rate: 0.0288675134595
Epoch:  11  avg train loss: 0.0879225121537  Reg Loss: 1.77075390171e-05 total time: 2.1500344276428223
avg loss validation: 0.129675089899 failed count: 0
learning rate: 0.0277350098113
Epoch:  12  avg train loss: 0.0830045459454  Reg Loss: 1.84350254072e-05 total time: 5.506231784820557
avg loss validation: 0.11229180779 failed count: 0
iter 0
iter 1
iter 2
returns [299.51413763953434, 534.28060898402055, 344.8380458486767]
mean return 392.877597491
std of return 101.684718316
learning rate: 0.0267261241912
Epoch:  13  avg train loss: 0.0870985310806  Reg Loss: 1.94034556889e-05 total time: 2.142723798751831
avg loss validation: 0.0665654726417 failed count: 0
learning rate: 0.0258198889747
Epoch:  14  avg train loss: 0.0787809611779  Reg Loss: 2.02369273532e-05 total time: 5.434162378311157
avg loss validation: 0.0615899105764 failed count: 0
iter 0
iter 1
iter 2
returns [395.03509697621729, 488.37104133747317, 291.8388304961037]
mean return 391.748322937
std of return 80.2675926189
learning rate: 0.025
Epoch:  15  avg train loss: 0.0815050455574  Reg Loss: 2.10432633308e-05 total time: 2.145297050476074
avg loss validation: 0.0895624109783 failed count: 0
learning rate: 0.0242535625036
Epoch:  16  avg train loss: 0.0748967824461  Reg Loss: 2.16683977675e-05 total time: 5.497231960296631
avg loss validation: 0.096539069566 failed count: 1
iter 0
100/1000
iter 1
iter 2
returns [645.64913770380679, 534.7382795930157, 551.0105041401838]
mean return 577.132640479
std of return 48.9018003547
learning rate: 0.0235702260396
Epoch:  17  avg train loss: 0.0824620303236  Reg Loss: 2.22792614336e-05 total time: 2.1587376594543457
avg loss validation: 0.100705769534 failed count: 0
learning rate: 0.0229415733871
Epoch:  18  avg train loss: 0.0778396847006  Reg Loss: 2.29684413536e-05 total time: 5.549753904342651
avg loss validation: 0.0892041764985 failed count: 0
iter 0
iter 1
iter 2
returns [417.91593269716856, 364.72351252028227, 345.14458735867112]
mean return 375.928010859
std of return 30.7470623297
learning rate: 0.022360679775
Epoch:  19  avg train loss: 0.0774282470466  Reg Loss: 2.36315820623e-05 total time: 2.185044527053833
avg loss validation: 0.0893159349285 failed count: 0
learning rate: 0.0218217890236
Epoch:  20  avg train loss: 0.0742859447365  Reg Loss: 2.42716583776e-05 total time: 5.53872275352478
avg loss validation: 0.0942562445399 failed count: 1
iter 0
100/1000
iter 1
100/1000
iter 2
returns [793.55929981857616, 642.13563614053953, 481.07679981691831]
mean return 638.923911925
std of return 127.590659483
learning rate: 0.0213200716356
Epoch:  21  avg train loss: 0.077892957912  Reg Loss: 2.4790233024e-05 total time: 2.218609094619751
avg loss validation: 0.0928386999783 failed count: 0
learning rate: 0.0208514414057
Epoch:  22  avg train loss: 0.0741611369268  Reg Loss: 2.53871650846e-05 total time: 5.70334267616272
avg loss validation: 0.0875396243221 failed count: 0
iter 0
iter 1
iter 2
returns [365.38051723573977, 614.19893865859615, 367.82399330546991]
mean return 449.134483067
std of return 116.722458618
learning rate: 0.0204124145232
Epoch:  23  avg train loss: 0.0748795631751  Reg Loss: 2.5963814109e-05 total time: 2.241849184036255
avg loss validation: 0.0847236660918 failed count: 0
learning rate: 0.02
Epoch:  24  avg train loss: 0.0720536298529  Reg Loss: 2.64496555748e-05 total time: 5.660998821258545
avg loss validation: 0.0910049665458 failed count: 1
iter 0
iter 1
iter 2
100/1000
returns [517.50967025412797, 293.00966253512235, 871.29257918384099]
mean return 560.603970658
std of return 238.041483734
learning rate: 0.0196116135138
Epoch:  25  avg train loss: 0.076277049254  Reg Loss: 2.69617817017e-05 total time: 2.2650630474090576
avg loss validation: 0.0748244555201 failed count: 0
learning rate: 0.019245008973
Epoch:  26  avg train loss: 0.0711535498884  Reg Loss: 2.74430649034e-05 total time: 5.694275379180908
avg loss validation: 0.0682462840668 failed count: 0
iter 0
100/1000
iter 1
iter 2
100/1000
returns [788.25271415069017, 474.32915985985568, 805.81113781987506]
mean return 689.464337277
std of return 152.292335071
learning rate: 0.0188982236505
Epoch:  27  avg train loss: 0.0785608790176  Reg Loss: 2.78681812883e-05 total time: 2.306612968444824
avg loss validation: 0.146551774807 failed count: 0
learning rate: 0.0185695338177
Epoch:  28  avg train loss: 0.0834494231003  Reg Loss: 2.85702964798e-05 total time: 5.784731149673462
avg loss validation: 0.127774063042 failed count: 0
iter 0
100/1000
iter 1
100/1000
iter 2
100/1000
returns [993.92271387361689, 680.70220198015318, 733.86148282204442]
mean return 802.828799559
std of return 136.855496421
learning rate: 0.0182574185835
Epoch:  29  avg train loss: 0.0854420011595  Reg Loss: 2.89894344261e-05 total time: 2.3311712741851807
avg loss validation: 0.0705850062824 failed count: 0
learning rate: 0.0179605302027
Epoch:  30  avg train loss: 0.0825273553327  Reg Loss: 2.95491043701e-05 total time: 5.800014495849609
avg loss validation: 0.0697050876637 failed count: 0
iter 0
100/1000
iter 1
100/1000
iter 2
100/1000
returns [681.00258238983224, 1401.2561325784616, 893.92433142989375]
mean return 992.061015466
std of return 302.119623885
learning rate: 0.0176776695297
Epoch:  31  avg train loss: 0.0819748739249  Reg Loss: 3.00598842551e-05 total time: 2.38828706741333
avg loss validation: 0.0794525141188 failed count: 0
learning rate: 0.0174077655956
Epoch:  32  avg train loss: 0.077295164744  Reg Loss: 3.04970983074e-05 total time: 5.9890172481536865
avg loss validation: 0.0833055301968 failed count: 1
iter 0
iter 1
iter 2
100/1000
200/1000
returns [432.70367254091548, 605.53842293110495, 2212.6978250185962]
mean return 1083.64664016
std of return 801.471728032
learning rate: 0.0171498585143
Epoch:  33  avg train loss: 0.0780991150248  Reg Loss: 3.09737792394e-05 total time: 2.4233596324920654
avg loss validation: 0.0712961678534 failed count: 0
learning rate: 0.0169030850946
Epoch:  34  avg train loss: 0.0766846838865  Reg Loss: 3.13764536918e-05 total time: 6.12121844291687
avg loss validation: 0.0631053705554 failed count: 0
iter 0
100/1000
iter 1
100/1000
iter 2
100/1000
returns [1146.5603688154461, 1016.0511939054456, 991.63059172646626]
mean return 1051.41405148
std of return 68.0132718873
learning rate: 0.0166666666667
Epoch:  35  avg train loss: 0.0824574965419  Reg Loss: 3.18940907126e-05 total time: 2.486973524093628
avg loss validation: 0.102971410212 failed count: 0
learning rate: 0.0164398987305
Epoch:  36  avg train loss: 0.0797865229586  Reg Loss: 3.24036901289e-05 total time: 6.218435049057007
avg loss validation: 0.112253405772 failed count: 1
iter 0
iter 1
iter 2
100/1000
returns [523.10528606917444, 495.58709482943306, 1315.9550488886482]
mean return 778.215809929
std of return 380.404985553
learning rate: 0.0162221421131
Epoch:  37  avg train loss: 0.0813181334161  Reg Loss: 3.2766199857e-05 total time: 2.5197856426239014
avg loss validation: 0.0807329162613 failed count: 0
learning rate: 0.0160128153805
Epoch:  38  avg train loss: 0.0781248730141  Reg Loss: 3.32256076574e-05 total time: 6.347489595413208
avg loss validation: 0.08308828237 failed count: 1
iter 0
100/1000
iter 1
iter 2
100/1000
returns [1404.5357063692009, 419.01514085855752, 674.91320055923984]
mean return 832.821349262
std of return 417.543557673
learning rate: 0.0158113883008
Epoch:  39  avg train loss: 0.0795139982267  Reg Loss: 3.35462403011e-05 total time: 2.572598457336426
avg loss validation: 0.0492608913838 failed count: 0
learning rate: 0.0156173761889
Epoch:  40  avg train loss: 0.0771329494724  Reg Loss: 3.39453189678e-05 total time: 6.298781394958496
avg loss validation: 0.051317396379 failed count: 1
iter 0
100/1000
200/1000
300/1000
400/1000
iter 1
100/1000
iter 2
100/1000
returns [3625.6277504179779, 1110.6999408210031, 1049.325659454292]
mean return 1928.5511169
std of return 1200.27594761
learning rate: 0.0154303349962
Epoch:  41  avg train loss: 0.083521749523  Reg Loss: 3.44824710814e-05 total time: 2.646716356277466
avg loss validation: 0.12442779904 failed count: 0
learning rate: 0.0152498570333
Epoch:  42  avg train loss: 0.0799588972265  Reg Loss: 3.49267861415e-05 total time: 6.543344974517822
avg loss validation: 0.123153013459 failed count: 0
iter 0
100/1000
200/1000
iter 1
100/1000
iter 2
100/1000
returns [1726.2459738714124, 1371.0710065987041, 1232.4156950585887]
mean return 1443.24422518
std of return 207.964445964
learning rate: 0.0150755672289
Epoch:  43  avg train loss: 0.0815040907755  Reg Loss: 3.5311155707e-05 total time: 2.6776504516601562
avg loss validation: 0.0710451815229 failed count: 0
learning rate: 0.01490711985
Epoch:  44  avg train loss: 0.0792739624485  Reg Loss: 3.57241532475e-05 total time: 6.633827209472656
avg loss validation: 0.0714907806342 failed count: 1
iter 0
100/1000
200/1000
300/1000
iter 1
100/1000
200/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
returns [3455.2142111631069, 2038.184799582848, 7237.7246663470478]
mean return 4243.70789236
std of return 2194.70505543
learning rate: 0.0147441956155
Epoch:  45  avg train loss: 0.0842082172059  Reg Loss: 3.63770907236e-05 total time: 2.87886118888855
avg loss validation: 0.129135967111 failed count: 0
learning rate: 0.0145864991498
Epoch:  46  avg train loss: 0.0827679102124  Reg Loss: 3.68282545348e-05 total time: 6.8597588539123535
avg loss validation: 0.123858388482 failed count: 0
iter 0
100/1000
iter 1
100/1000
200/1000
iter 2
100/1000
returns [837.10584267716285, 1950.4260753846986, 711.86737927425315]
mean return 1166.46643245
std of return 556.696032856
learning rate: 0.0144337567297
Epoch:  47  avg train loss: 0.0808552139474  Reg Loss: 3.72957660141e-05 total time: 2.8898513317108154
avg loss validation: 0.0723875035208 failed count: 0
learning rate: 0.0142857142857
Epoch:  48  avg train loss: 0.0776906913925  Reg Loss: 3.76732647673e-05 total time: 7.263135194778442
avg loss validation: 0.0597968316465 failed count: 0
iter 0
100/1000
iter 1
100/1000
200/1000
iter 2
100/1000
200/1000
returns [849.72615429628718, 1494.7779217212671, 2288.0361333536416]
mean return 1544.18006979
std of return 588.225766261
learning rate: 0.0141421356237
Epoch:  49  avg train loss: 0.0917849465745  Reg Loss: 3.81222160865e-05 total time: 2.9957385063171387
avg loss validation: 0.0805378107269 failed count: 0
learning rate: 0.0140028008403
Epoch:  50  avg train loss: 0.113320650854  Reg Loss: 3.91501741559e-05 total time: 7.326236724853516
avg loss validation: 0.078862249577 failed count: 0
iter 0
100/1000
200/1000
300/1000
iter 1
100/1000
iter 2
100/1000
returns [2986.3615107429646, 1087.404298265106, 1060.7701002061842]
mean return 1711.51196974
std of return 901.520330433
learning rate: 0.0138675049056
Epoch:  51  avg train loss: 0.0941983229017  Reg Loss: 3.95672755673e-05 total time: 3.0311055183410645
avg loss validation: 0.0926618581628 failed count: 0
learning rate: 0.0137360563949
Epoch:  52  avg train loss: 0.0824863406325  Reg Loss: 3.99409684468e-05 total time: 7.514132261276245
avg loss validation: 0.0868800688102 failed count: 0
iter 0
100/1000
iter 1
100/1000
200/1000
300/1000
iter 2
100/1000
200/1000
returns [710.27608312953885, 2975.6812640534849, 1981.4750162692305]
mean return 1889.14412115
std of return 927.149360166
learning rate: 0.0136082763488
Epoch:  53  avg train loss: 0.0793954244633  Reg Loss: 4.0344448744e-05 total time: 3.113330125808716
avg loss validation: 0.0954462266824 failed count: 0
learning rate: 0.0134839972493
Epoch:  54  avg train loss: 0.0771333540278  Reg Loss: 4.06187241636e-05 total time: 7.466585159301758
avg loss validation: 0.104080544882 failed count: 1
iter 0
100/1000
iter 1
100/1000
200/1000
300/1000
400/1000
iter 2
100/1000
200/1000
300/1000
returns [1516.4432283465289, 3972.8660325748488, 3402.8207657895928]
mean return 2964.04334224
std of return 1049.7293203
learning rate: 0.0133630620956
Epoch:  55  avg train loss: 0.0762926453228  Reg Loss: 4.09232454629e-05 total time: 3.254831314086914
avg loss validation: 0.0786112848981 failed count: 0
learning rate: 0.0132453235707
Epoch:  56  avg train loss: 0.0756159986529  Reg Loss: 4.11590178082e-05 total time: 7.773322582244873
avg loss validation: 0.0740624544532 failed count: 0
iter 0
100/1000
iter 1
100/1000
iter 2
100/1000
returns [1010.3582003192895, 1056.7909105689362, 937.93056394099904]
mean return 1001.69322494
std of return 48.9098275662
learning rate: 0.013130643286
Epoch:  57  avg train loss: 0.0761719558856  Reg Loss: 4.11641887221e-05 total time: 3.2466437816619873
avg loss validation: 0.0488152533768 failed count: 0
learning rate: 0.0130188910981
Epoch:  58  avg train loss: 0.0748052096287  Reg Loss: 4.14234431113e-05 total time: 7.654398441314697
avg loss validation: 0.0462417484216 failed count: 0
iter 0
100/1000
iter 1
100/1000
200/1000
300/1000
400/1000
iter 2
100/1000
returns [1243.491919351776, 4091.116313397667, 1259.1568797262412]
mean return 2197.92170416
std of return 1338.70602177
learning rate: 0.0129099444874
Epoch:  59  avg train loss: 0.0759701524973  Reg Loss: 4.17033502445e-05 total time: 3.3441576957702637
avg loss validation: 0.0768626844262 failed count: 0
learning rate: 0.0128036879933
Epoch:  60  avg train loss: 0.0755663598431  Reg Loss: 4.19542682295e-05 total time: 7.950808048248291
avg loss validation: 0.069534056575 failed count: 0
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
iter 1
100/1000
iter 2
100/1000
200/1000
returns [4867.6051732940487, 1131.9914328600535, 1821.3602713285895]
mean return 2606.98562583
std of return 1623.08510819
learning rate: 0.0127000127
Epoch:  61  avg train loss: 0.0744616270628  Reg Loss: 4.24161287777e-05 total time: 3.4517812728881836
avg loss validation: 0.0594630796655 failed count: 0
learning rate: 0.012598815767
Epoch:  62  avg train loss: 0.0729504433558  Reg Loss: 4.26604430496e-05 total time: 8.174129486083984
avg loss validation: 0.0572444036087 failed count: 0
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
iter 1
100/1000
200/1000
iter 2
100/1000
returns [5618.617645005178, 1790.8728825873329, 1522.1707533864142]
mean return 2977.22042699
std of return 1870.9684936
learning rate: 0.0125
Epoch:  63  avg train loss: 0.0741461862807  Reg Loss: 4.28813570404e-05 total time: 3.5754997730255127
avg loss validation: 0.061615299848 failed count: 0
learning rate: 0.0124034734589
Epoch:  64  avg train loss: 0.0750738736907  Reg Loss: 4.31592149613e-05 total time: 8.407675981521606
avg loss validation: 0.0578321590407 failed count: 0
iter 0
100/1000
200/1000
300/1000
iter 1
100/1000
iter 2
100/1000
200/1000
returns [3223.9309653544756, 696.85140314884779, 1752.3901270937899]
mean return 1891.05749853
std of return 1036.32499997
learning rate: 0.0123091490979
Epoch:  65  avg train loss: 0.0751834130213  Reg Loss: 4.3275364926e-05 total time: 3.604397773742676
avg loss validation: 0.0663144377133 failed count: 0
learning rate: 0.0122169444356
Epoch:  66  avg train loss: 0.0722756579209  Reg Loss: 4.35010584404e-05 total time: 8.641528367996216
avg loss validation: 0.0710596765153 failed count: 1
iter 0
100/1000
200/1000
300/1000
400/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
returns [3590.6867980548309, 4836.5663343375909, 5463.8634023251816]
mean return 4630.37217824
std of return 778.496272825
learning rate: 0.0121267812518
Epoch:  67  avg train loss: 0.0763158533371  Reg Loss: 4.37665287056e-05 total time: 3.7921407222747803
avg loss validation: 0.047746096066 failed count: 0
learning rate: 0.0120385853086
Epoch:  68  avg train loss: 0.0746643015263  Reg Loss: 4.40563842679e-05 total time: 8.708652973175049
avg loss validation: 0.0515460410888 failed count: 1
iter 0
100/1000
iter 1
100/1000
200/1000
300/1000
iter 2
100/1000
200/1000
returns [1090.8263791786601, 3409.8993273238666, 2583.1058856638147]
mean return 2361.27719739
std of return 959.663416659
learning rate: 0.0119522860933
Epoch:  69  avg train loss: 0.0746313301659  Reg Loss: 4.45983554087e-05 total time: 3.900428295135498
avg loss validation: 0.0572210146109 failed count: 0
learning rate: 0.0118678165819
Epoch:  70  avg train loss: 0.0756957442745  Reg Loss: 4.49193438173e-05 total time: 9.022381067276001
avg loss validation: 0.0625717618962 failed count: 1
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [8645.2956100310184, 10064.712654622377, 10153.250180542336]
mean return 9621.0861484
std of return 690.934200654
learning rate: 0.0117851130198
Epoch:  71  avg train loss: 0.0747680963248  Reg Loss: 4.51240619723e-05 total time: 4.17045521736145
avg loss validation: 0.0709241822621 failed count: 0
learning rate: 0.0117041147196
Epoch:  72  avg train loss: 0.0719576856832  Reg Loss: 4.5366161956e-05 total time: 9.563727140426636
avg loss validation: 0.0697586767822 failed count: 0
iter 0
100/1000
200/1000
iter 1
100/1000
iter 2
100/1000
200/1000
300/1000
400/1000
returns [2510.6878291716239, 1442.9306701462704, 4719.3403359778395]
mean return 2890.98627843
std of return 1364.352168
learning rate: 0.0116247638744
Epoch:  73  avg train loss: 0.0713587024347  Reg Loss: 4.537736473e-05 total time: 4.2399070262908936
avg loss validation: 0.0797060334153 failed count: 0
learning rate: 0.0115470053838
Epoch:  74  avg train loss: 0.0696547392487  Reg Loss: 4.55588614911e-05 total time: 9.75603723526001
avg loss validation: 0.0815139295936 failed count: 1
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [9923.7475433159252, 4040.0073663838457, 10233.833394642925]
mean return 8065.86276811
std of return 2849.52301219
learning rate: 0.0114707866935
Epoch:  75  avg train loss: 0.0693755933412  Reg Loss: 4.59117186099e-05 total time: 4.501619577407837
avg loss validation: 0.0702883808135 failed count: 0
learning rate: 0.011396057646
Epoch:  76  avg train loss: 0.0683619460964  Reg Loss: 4.60617873043e-05 total time: 10.173388242721558
avg loss validation: 0.074063174549 failed count: 1
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
returns [10040.91503607802, 10192.500759748731, 4601.3195939660536]
mean return 8278.24512993
std of return 2600.71536385
learning rate: 0.0113227703414
Epoch:  77  avg train loss: 0.0676737289626  Reg Loss: 4.63186374352e-05 total time: 4.760580539703369
avg loss validation: 0.0715680314421 failed count: 0
learning rate: 0.0112508790093
Epoch:  78  avg train loss: 0.0666335903375  Reg Loss: 4.64702977127e-05 total time: 10.965228796005249
avg loss validation: 0.0698385242929 failed count: 0
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
iter 2
100/1000
200/1000
returns [10086.131104833787, 5395.623669300614, 2546.6424371085909]
mean return 6009.46573708
std of return 3108.43732507
learning rate: 0.0111803398875
Epoch:  79  avg train loss: 0.066806384515  Reg Loss: 4.65651359782e-05 total time: 4.929062604904175
avg loss validation: 0.0612532888435 failed count: 0
learning rate: 0.0111111111111
Epoch:  80  avg train loss: 0.0657709473019  Reg Loss: 4.67030294988e-05 total time: 11.135726690292358
avg loss validation: 0.0637668406696 failed count: 1
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
returns [6366.0015112844776, 10241.268033267786, 3560.2897025714028]
mean return 6722.51974904
std of return 2739.12355863
learning rate: 0.0110431526075
Epoch:  81  avg train loss: 0.0669921557187  Reg Loss: 4.68603164616e-05 total time: 5.162345886230469
avg loss validation: 0.0579521598227 failed count: 0
learning rate: 0.010976425999
Epoch:  82  avg train loss: 0.0650455150506  Reg Loss: 4.69888202146e-05 total time: 11.764635801315308
avg loss validation: 0.0578438669522 failed count: 0
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
iter 1
100/1000
200/1000
300/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
returns [6370.9161128865253, 3379.7692799736992, 5417.232931501866]
mean return 5055.97277479
std of return 1247.56336369
learning rate: 0.0109108945118
Epoch:  83  avg train loss: 0.0653178422915  Reg Loss: 4.7108469275e-05 total time: 5.348273515701294
avg loss validation: 0.184083724745 failed count: 0
learning rate: 0.0108465228909
Epoch:  84  avg train loss: 0.0640046501241  Reg Loss: 4.72343081208e-05 total time: 11.994563579559326
avg loss validation: 0.448011642049 failed count: 1
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [10428.578790880174, 10354.182315856937, 10355.329364492487]
mean return 10379.3634904
std of return 34.8036231872
learning rate: 0.0107832773203
Epoch:  85  avg train loss: 0.0629155208138  Reg Loss: 4.7441191763e-05 total time: 5.582376480102539
avg loss validation: 0.0564540946313 failed count: 0
learning rate: 0.0107211253484
Epoch:  86  avg train loss: 0.0607829268474  Reg Loss: 4.74918848783e-05 total time: 12.358521223068237
avg loss validation: 0.0535681729102 failed count: 0
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
iter 2
100/1000
200/1000
300/1000
returns [10291.412081552558, 4442.4856619204302, 3778.5473945509748]
mean return 6170.81504601
std of return 2926.28244851
learning rate: 0.0106600358178
Epoch:  87  avg train loss: 0.0590335952143  Reg Loss: 4.74503979408e-05 total time: 5.809722185134888
avg loss validation: 0.0710897581926 failed count: 0
learning rate: 0.0105999788001
Epoch:  88  avg train loss: 0.0581728879361  Reg Loss: 4.74357435629e-05 total time: 12.842638492584229
avg loss validation: 0.0668402659873 failed count: 0
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
iter 1
100/1000
200/1000
300/1000
iter 2
100/1000
200/1000
300/1000
returns [6391.0949625045159, 3408.4159589243986, 2779.3440408491724]
mean return 4192.95165409
std of return 1575.39590215
learning rate: 0.0105409255339
Epoch:  89  avg train loss: 0.0577227843452  Reg Loss: 4.74211459495e-05 total time: 5.9024434089660645
avg loss validation: 0.0544618355311 failed count: 0
learning rate: 0.0104828483672
Epoch:  90  avg train loss: 0.0578897901967  Reg Loss: 4.74115955794e-05 total time: 13.011069536209106
avg loss validation: 0.056171526052 failed count: 1
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [10382.018058897782, 10409.174926658117, 10210.269690437111]
mean return 10333.820892
std of return 88.0645536787
learning rate: 0.0104257207029
Epoch:  91  avg train loss: 0.0563308315536  Reg Loss: 4.75063072459e-05 total time: 6.282153606414795
avg loss validation: 0.0441180631427 failed count: 0
learning rate: 0.0103695169473
Epoch:  92  avg train loss: 0.055112041529  Reg Loss: 4.74400266351e-05 total time: 13.711615324020386
avg loss validation: 0.0459092443766 failed count: 1
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [10369.497141257201, 10287.86456291275, 10302.387015923081]
mean return 10319.91624
std of return 35.5567596495
learning rate: 0.0103142124626
Epoch:  93  avg train loss: 0.0529017642483  Reg Loss: 4.72302364315e-05 total time: 6.5482261180877686
avg loss validation: 0.044270043541 failed count: 0
learning rate: 0.0102597835209
Epoch:  94  avg train loss: 0.0530653896754  Reg Loss: 4.71554656216e-05 total time: 14.284949541091919
avg loss validation: 0.0455455513933 failed count: 1
iter 0
100/1000
200/1000
300/1000
iter 1
100/1000
200/1000
iter 2
100/1000
200/1000
300/1000
returns [2840.5636712903042, 2598.5526033662295, 2983.5307513245343]
mean return 2807.54900866
std of return 158.890988968
learning rate: 0.0102062072616
Epoch:  95  avg train loss: 0.0529527532865  Reg Loss: 4.72137433117e-05 total time: 6.587095022201538
avg loss validation: 0.0464499081774 failed count: 0
learning rate: 0.0101534616513
Epoch:  96  avg train loss: 0.0522495055924  Reg Loss: 4.71322596816e-05 total time: 14.302327394485474
avg loss validation: 0.0448466904883 failed count: 0
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [9978.3271996902495, 10283.586407870072, 10363.285441817785]
mean return 10208.3996831
std of return 165.907594218
learning rate: 0.0101015254455
Epoch:  97  avg train loss: 0.0518040146932  Reg Loss: 4.71616776728e-05 total time: 6.897024631500244
avg loss validation: 0.0524377353139 failed count: 0
learning rate: 0.0100503781526
Epoch:  98  avg train loss: 0.0523466679602  Reg Loss: 4.70923054451e-05 total time: 15.047100067138672
avg loss validation: 0.0479004942531 failed count: 0
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [10368.837845605791, 10277.262830507778, 10351.44973606195]
mean return 10332.5168041
std of return 39.7100976515
learning rate: 0.01
Epoch:  99  avg train loss: 0.0501709694127  Reg Loss: 4.69265662172e-05 total time: 7.2357048988342285
avg loss validation: 0.045600678637 failed count: 0
learning rate: 0.0099503719021
Epoch:  100  avg train loss: 0.0500602686701  Reg Loss: 4.68318347876e-05 total time: 15.682249069213867
avg loss validation: 0.0579956459138 failed count: 1
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [10224.432329637832, 10321.748104299561, 10288.898866058176]
mean return 10278.3597667
std of return 40.4218955643
learning rate: 0.00990147542977
Epoch:  101  avg train loss: 0.0484641499845  Reg Loss: 4.67957620149e-05 total time: 7.55247688293457
avg loss validation: 0.0492811625919 failed count: 0
learning rate: 0.00985329278164
Epoch:  102  avg train loss: 0.0480229688759  Reg Loss: 4.66644793708e-05 total time: 16.305262565612793
avg loss validation: 0.0515975093574 failed count: 1
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
returns [5129.114294009617, 10325.433420765909, 8154.7166038960559]
mean return 7869.75477289
std of return 2130.93650001
learning rate: 0.00980580675691
Epoch:  103  avg train loss: 0.0479528974681  Reg Loss: 4.65570997012e-05 total time: 7.7225189208984375
avg loss validation: 0.0525588334604 failed count: 0
learning rate: 0.00975900072949
Epoch:  104  avg train loss: 0.0466889712075  Reg Loss: 4.64124793898e-05 total time: 16.57265877723694
avg loss validation: 0.0451913746774 failed count: 0
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [10192.957999225944, 10095.249462330545, 10227.37932058005]
mean return 10171.8622607
std of return 55.9663440402
learning rate: 0.00971285862357
Epoch:  105  avg train loss: 0.0462294904219  Reg Loss: 4.63363990985e-05 total time: 8.053334712982178
avg loss validation: 0.0452298301841 failed count: 0
learning rate: 0.00966736489046
Epoch:  106  avg train loss: 0.0459262671076  Reg Loss: 4.61992606408e-05 total time: 17.317660331726074
avg loss validation: 0.0497264816589 failed count: 1
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [10285.316739634478, 10265.188983907881, 10330.804907284215]
mean return 10293.7702103
std of return 27.4464111138
learning rate: 0.00962250448649
Epoch:  107  avg train loss: 0.0449481042165  Reg Loss: 4.59659702516e-05 total time: 8.356717586517334
avg loss validation: 0.0358057610071 failed count: 0
learning rate: 0.00957826285221
Epoch:  108  avg train loss: 0.0444978843208  Reg Loss: 4.58256937389e-05 total time: 17.951927185058594
avg loss validation: 0.0330988225653 failed count: 0
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [10206.992149315118, 8144.0838617312547, 10260.725411146506]
mean return 9537.26714073
std of return 985.373551002
learning rate: 0.00953462589246
Epoch:  109  avg train loss: 0.0444128166188  Reg Loss: 4.57041828568e-05 total time: 8.669575452804565
avg loss validation: 0.0490291935277 failed count: 0
learning rate: 0.00949157995752
Epoch:  110  avg train loss: 0.0438721066131  Reg Loss: 4.5563423141e-05 total time: 18.426039934158325
avg loss validation: 0.0461033172959 failed count: 0
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [7734.4897906391398, 10217.024601791732, 10385.608165274263]
mean return 9445.70751924
std of return 1211.96938643
learning rate: 0.00944911182523
Epoch:  111  avg train loss: 0.0431653050596  Reg Loss: 4.54587086936e-05 total time: 8.902707815170288
avg loss validation: 0.0399946694271 failed count: 0
learning rate: 0.00940720868384
Epoch:  112  avg train loss: 0.0426838819998  Reg Loss: 4.53097572789e-05 total time: 19.041746854782104
avg loss validation: 0.0392224531806 failed count: 0
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [10449.976384765381, 10356.184826773124, 10332.304140821923]
mean return 10379.4884508
std of return 50.7870286445
learning rate: 0.00936585811582
Epoch:  113  avg train loss: 0.0417401996432  Reg Loss: 4.51617191698e-05 total time: 9.25647258758545
avg loss validation: 0.0337983929774 failed count: 0
learning rate: 0.0093250480824
Epoch:  114  avg train loss: 0.0411548831107  Reg Loss: 4.49737418994e-05 total time: 19.604511499404907
avg loss validation: 0.0354185312569 failed count: 1
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [10290.324126887521, 10293.118585159089, 10373.947265967554]
mean return 10319.1299927
std of return 38.7784505276
learning rate: 0.00928476690885
Epoch:  115  avg train loss: 0.0405888722272  Reg Loss: 4.48234915828e-05 total time: 9.559654951095581
avg loss validation: 0.0429982653707 failed count: 0
learning rate: 0.00924500327042
Epoch:  116  avg train loss: 0.0408598828341  Reg Loss: 4.46559665603e-05 total time: 20.323790788650513
avg loss validation: 0.048984048446 failed count: 1
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [10245.753127732027, 10312.60437197212, 10406.589981002615]
mean return 10321.6491602
std of return 65.9721127373
learning rate: 0.00920574617898
Epoch:  117  avg train loss: 0.0397389458031  Reg Loss: 4.43876765609e-05 total time: 9.855390548706055
avg loss validation: 0.0407266135351 failed count: 0
learning rate: 0.00916698497028
Epoch:  118  avg train loss: 0.0394096286273  Reg Loss: 4.42032177376e-05 total time: 20.945719003677368
avg loss validation: 0.0437532992744 failed count: 1
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [10354.204846882585, 2229.845052528673, 10292.5139236305]
mean return 7625.52127435
std of return 3815.40236939
learning rate: 0.00912870929175
Epoch:  119  avg train loss: 0.0395475311485  Reg Loss: 4.40397396438e-05 total time: 10.071866512298584
avg loss validation: 0.0355706104096 failed count: 0
learning rate: 0.00909090909091
Epoch:  120  avg train loss: 0.0389963152474  Reg Loss: 4.38658208409e-05 total time: 21.24556279182434
avg loss validation: 0.0363509498624 failed count: 1
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [10352.023082096506, 10230.614315099792, 10330.32065268843]
mean return 10304.31935
std of return 52.8650671271
learning rate: 0.00905357460425
Epoch:  121  avg train loss: 0.0380829235262  Reg Loss: 4.36904146782e-05 total time: 10.374371528625488
avg loss validation: 0.0339231176269 failed count: 0
learning rate: 0.00901669634667
Epoch:  122  avg train loss: 0.0377175648647  Reg Loss: 4.34995112754e-05 total time: 22.00061535835266
avg loss validation: 0.0346126674971 failed count: 1
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
returns [10399.833260356934, 8742.3449216578156, 7965.4795333224556]
mean return 9035.88590511
std of return 1015.26490757
learning rate: 0.00898026510134
Epoch:  123  avg train loss: 0.0374429875096  Reg Loss: 4.32805726757e-05 total time: 10.646109580993652
avg loss validation: 0.0384141980023 failed count: 0
learning rate: 0.00894427191
Epoch:  124  avg train loss: 0.037421476281  Reg Loss: 4.30938357545e-05 total time: 22.530584812164307
avg loss validation: 0.0634945447595 failed count: 1
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [10205.01185106059, 10344.210635469532, 10488.423320264019]
mean return 10345.8819356
std of return 115.708283057
learning rate: 0.00890870806375
Epoch:  125  avg train loss: 0.0366366595602  Reg Loss: 4.29516588444e-05 total time: 10.986602306365967
avg loss validation: 0.0360630101786 failed count: 0
learning rate: 0.00887356509416
Epoch:  126  avg train loss: 0.0369103046829  Reg Loss: 4.28012725358e-05 total time: 23.066083669662476
avg loss validation: 0.0353775225841 failed count: 0
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [10332.880594280694, 10394.07590990986, 10394.384392471702]
mean return 10373.7802989
std of return 28.9207326793
learning rate: 0.00883883476483
Epoch:  127  avg train loss: 0.0357853298542  Reg Loss: 4.2623743643e-05 total time: 11.28607702255249
avg loss validation: 0.0352765471932 failed count: 0
learning rate: 0.00880450906326
Epoch:  128  avg train loss: 0.0358105791262  Reg Loss: 4.2415239373e-05 total time: 23.779773235321045
avg loss validation: 0.0408495454531 failed count: 1
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [10465.494872264098, 10397.043789340387, 10436.1698057084]
mean return 10432.9028224
std of return 28.0403586892
learning rate: 0.00877058019307
Epoch:  129  avg train loss: 0.0352430267193  Reg Loss: 4.21771479168e-05 total time: 11.579702138900757
avg loss validation: 0.0359344749892 failed count: 0
learning rate: 0.00873704056661
Epoch:  130  avg train loss: 0.0349569402547  Reg Loss: 4.19860987252e-05 total time: 24.39483118057251
avg loss validation: 0.0383785101219 failed count: 1
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [10508.189318094766, 10428.9741224918, 10436.19936941663]
mean return 10457.7876033
std of return 35.7612517153
learning rate: 0.00870388279778
Epoch:  131  avg train loss: 0.034739118263  Reg Loss: 4.18197135086e-05 total time: 11.892111778259277
avg loss validation: 0.0293788944313 failed count: 0
learning rate: 0.00867109969524
Epoch:  132  avg train loss: 0.0346283270135  Reg Loss: 4.16386834737e-05 total time: 25.02639365196228
avg loss validation: 0.0276326855183 failed count: 0
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [10402.455475551031, 10486.885165204121, 10471.128607082048]
mean return 10453.4897493
std of return 36.6555143844
learning rate: 0.00863868425581
Epoch:  133  avg train loss: 0.0340266868996  Reg Loss: 4.14841011452e-05 total time: 12.144251108169556
avg loss validation: 0.035867921195 failed count: 0
learning rate: 0.00860662965824
Epoch:  134  avg train loss: 0.0338440557376  Reg Loss: 4.12966442846e-05 total time: 25.548402070999146
avg loss validation: 0.0353919919305 failed count: 0
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [10177.313478123418, 10182.765322591544, 10232.191017703826]
mean return 10197.4232728
std of return 24.6850523728
learning rate: 0.00857492925713
Epoch:  135  avg train loss: 0.0334005467318  Reg Loss: 4.10489971709e-05 total time: 12.512993097305298
avg loss validation: 0.0359682909759 failed count: 0
learning rate: 0.00854357657717
Epoch:  136  avg train loss: 0.03301262096  Reg Loss: 4.08518159734e-05 total time: 26.25366234779358
avg loss validation: 0.0376447342889 failed count: 1
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [10265.015926390994, 10401.526521093976, 10469.285164619956]
mean return 10378.609204
std of return 84.9524690147
learning rate: 0.00851256530759
Epoch:  137  avg train loss: 0.0324639806526  Reg Loss: 4.06795348681e-05 total time: 12.971415758132935
avg loss validation: 0.0345789497979 failed count: 0
learning rate: 0.0084818892968
Epoch:  138  avg train loss: 0.03302913668  Reg Loss: 4.05118869326e-05 total time: 26.85942244529724
avg loss validation: 0.036133656056 failed count: 1
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [10327.23088810397, 10387.336323218884, 10415.663346768548]
mean return 10376.7435194
std of return 36.8712223511
learning rate: 0.00845154254729
Epoch:  139  avg train loss: 0.0325069946531  Reg Loss: 4.03540465405e-05 total time: 13.116991996765137
avg loss validation: 0.0300106980394 failed count: 0
learning rate: 0.00842151921067
Epoch:  140  avg train loss: 0.0320537170889  Reg Loss: 4.01870082535e-05 total time: 27.422160387039185
avg loss validation: 0.0288875583127 failed count: 0
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [10314.780709963392, 10455.142732657736, 10271.810996257565]
mean return 10347.244813
std of return 78.2860987251
learning rate: 0.00839181358297
Epoch:  141  avg train loss: 0.0315870669919  Reg Loss: 3.99240962843e-05 total time: 13.44701862335205
avg loss validation: 0.0276187912602 failed count: 0
learning rate: 0.00836242010007
Epoch:  142  avg train loss: 0.0313158763256  Reg Loss: 3.97211617326e-05 total time: 28.0872905254364
avg loss validation: 0.0265641632561 failed count: 0
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [10200.558306712979, 10373.320910166736, 10341.845418593357]
mean return 10305.2415452
std of return 75.1292750349
learning rate: 0.00833333333333
Epoch:  143  avg train loss: 0.031370289274  Reg Loss: 3.95665854252e-05 total time: 13.754957675933838
avg loss validation: 0.034320612848 failed count: 0
learning rate: 0.00830454798537
Epoch:  144  avg train loss: 0.0309052528845  Reg Loss: 3.93908544984e-05 total time: 28.689189672470093
avg loss validation: 0.0333045428549 failed count: 0
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [10313.148963032949, 10219.35182948363, 10435.378332452465]
mean return 10322.626375
std of return 88.4467014621
learning rate: 0.00827605888602
Epoch:  145  avg train loss: 0.03059248809  Reg Loss: 3.92171975891e-05 total time: 14.016973495483398
avg loss validation: 0.0294431576133 failed count: 0
learning rate: 0.00824786098842
Epoch:  146  avg train loss: 0.0303990081773  Reg Loss: 3.9036492566e-05 total time: 29.28395175933838
avg loss validation: 0.0267052124964 failed count: 0
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [10391.41617631535, 10357.504049284364, 10371.953490449925]
mean return 10373.624572
std of return 13.894902546
learning rate: 0.00821994936527
Epoch:  147  avg train loss: 0.0300734494645  Reg Loss: 3.88671499346e-05 total time: 14.340957164764404
avg loss validation: 0.0293414701702 failed count: 0
learning rate: 0.00819231920519
Epoch:  148  avg train loss: 0.0303113239995  Reg Loss: 3.8731327207e-05 total time: 29.863637685775757
avg loss validation: 0.0291056575103 failed count: 0
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [10411.192574487803, 10509.598567784145, 10388.742746873231]
mean return 10436.5112964
std of return 52.4868911937
learning rate: 0.00816496580928
Epoch:  149  avg train loss: 0.0295277524387  Reg Loss: 3.84706797092e-05 total time: 14.5850088596344
avg loss validation: 0.0313451544503 failed count: 0
learning rate: 0.00813788458771
Epoch:  150  avg train loss: 0.029468961484  Reg Loss: 3.82848916726e-05 total time: 30.43860173225403
avg loss validation: 0.0290710550846 failed count: 0
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [10421.82729188648, 10295.655723731084, 10369.530817398172]
mean return 10362.3379443
std of return 51.75982496
learning rate: 0.00811107105654
Epoch:  151  avg train loss: 0.0286972702391  Reg Loss: 3.81110347649e-05 total time: 14.953590154647827
avg loss validation: 0.0271558771177 failed count: 0
learning rate: 0.00808452083454
Epoch:  152  avg train loss: 0.0288912654036  Reg Loss: 3.79184561653e-05 total time: 31.111843585968018
avg loss validation: 0.0285261035257 failed count: 1
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [10434.518902327309, 10445.205141800989, 10353.081529156922]
mean return 10410.9351911
std of return 41.1406820415
learning rate: 0.00805822964025
Epoch:  153  avg train loss: 0.0283895614233  Reg Loss: 3.77353438771e-05 total time: 15.351912498474121
avg loss validation: 0.0242040220126 failed count: 0
learning rate: 0.00803219328902
Epoch:  154  avg train loss: 0.0290673145234  Reg Loss: 3.75653065221e-05 total time: 31.80953025817871
avg loss validation: 0.0262481365918 failed count: 1
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [10370.960261998234, 10388.988831655046, 10359.060434439814]
mean return 10373.003176
std of return 12.3033156311
learning rate: 0.00800640769025
Epoch:  155  avg train loss: 0.0280738736391  Reg Loss: 3.7336388699e-05 total time: 15.510862827301025
avg loss validation: 0.0284067976253 failed count: 0
learning rate: 0.00798086884468
Epoch:  156  avg train loss: 0.0276170734563  Reg Loss: 3.71365831917e-05 total time: 32.269604206085205
avg loss validation: 0.0305316299115 failed count: 1
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [10365.484022270402, 10436.658939572699, 10366.767100796784]
mean return 10389.6366875
std of return 33.2538790903
learning rate: 0.00795557284176
Epoch:  157  avg train loss: 0.0276265810215  Reg Loss: 3.69553041361e-05 total time: 15.87100100517273
avg loss validation: 0.0278505801367 failed count: 0
learning rate: 0.00793051585718
Epoch:  158  avg train loss: 0.0278203005603  Reg Loss: 3.67791144049e-05 total time: 32.968385457992554
avg loss validation: 0.0285499743824 failed count: 1
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [10539.027958588289, 10437.959616998607, 10349.7705200941]
mean return 10442.2526986
std of return 77.3236376895
learning rate: 0.00790569415042
Epoch:  159  avg train loss: 0.0273179434952  Reg Loss: 3.66298857295e-05 total time: 16.159374713897705
avg loss validation: 0.0265387274161 failed count: 0
learning rate: 0.00788110406239
Epoch:  160  avg train loss: 0.0270329483588  Reg Loss: 3.64484105211e-05 total time: 33.541032552719116
avg loss validation: 0.0279262840443 failed count: 1
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [10285.05248605349, 10283.242289003079, 10360.830487853273]
mean return 10309.708421
std of return 36.1563133736
learning rate: 0.00785674201318
Epoch:  161  avg train loss: 0.0271514430497  Reg Loss: 3.6301841162e-05 total time: 16.5228910446167
avg loss validation: 0.0246466562476 failed count: 0
learning rate: 0.00783260449988
Epoch:  162  avg train loss: 0.0265859014406  Reg Loss: 3.61300051349e-05 total time: 34.92558789253235
avg loss validation: 0.0294375594134 failed count: 1
iter 0
100/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
