3.6.0 |Anaconda custom (64-bit)| (default, Dec 23 2016, 12:22:00) 
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]
loading and building expert policy
obs (1, 376) (1, 376)
loaded and built
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 3
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 4
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 5
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 6
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 7
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 8
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
iter 9
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 10
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 11
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 12
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 13
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 14
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 15
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
iter 16
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 17
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 18
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 19
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [9718.462338573132, 9541.5637142359483, 9781.371435724921, 9814.130616956676, 9658.6847989148609, 9582.895583394351, 9884.8702346537611, 9542.2752975757558, 7466.2363162550319, 9822.7355037406123, 9752.4505848294666, 9642.16904030158, 9776.966310421145, 9781.2036261633366, 9757.6272604464666, 7146.115537972546, 9478.9953986758792, 9679.9960682168166, 9501.7596008740966, 9647.2160066094511]
mean return 9448.88626373
std of return 724.525886195
learning rate: 0.0707106781187
Epoch:  1  avg train loss: 1.52805184963  Reg Loss: 7.64971106556e-06 total time: 44.571518898010254
avg loss validation: 0.322172789905 failed count: 0
learning rate: 0.057735026919
Epoch:  2  avg train loss: 0.307708441838  Reg Loss: 6.99053875396e-06 total time: 92.49247026443481
avg loss validation: 0.297748376973 failed count: 0
learning rate: 0.05
Epoch:  3  avg train loss: 0.286078677368  Reg Loss: 6.48767390127e-06 total time: 141.07555294036865
avg loss validation: 0.277689229523 failed count: 0
learning rate: 0.04472135955
Epoch:  4  avg train loss: 0.27003373665  Reg Loss: 6.13829381133e-06 total time: 189.36437225341797
avg loss validation: 0.263799594661 failed count: 0
learning rate: 0.0408248290464
Epoch:  5  avg train loss: 0.257418043905  Reg Loss: 5.87520459528e-06 total time: 237.78793025016785
avg loss validation: 0.252146676593 failed count: 0
iter 0
100/1000
iter 1
iter 2
100/1000
iter 3
100/1000
200/1000
iter 4
100/1000
iter 5
100/1000
iter 6
100/1000
iter 7
100/1000
iter 8
100/1000
iter 9
100/1000
iter 10
100/1000
iter 11
100/1000
iter 12
100/1000
iter 13
iter 14
100/1000
iter 15
100/1000
iter 16
100/1000
iter 17
100/1000
iter 18
iter 19
returns [925.64190339207016, 474.14005080300518, 644.67416419237236, 1511.7361742168484, 1081.9424556647077, 664.02081118397234, 1002.725577820308, 826.47282834270527, 860.91739134781017, 1170.9633328882917, 826.40883748482975, 900.60710510930346, 1159.5717665573243, 534.19663880924611, 759.84993401573752, 779.08931463226509, 744.90894218703602, 961.25246133068686, 367.64037407317323, 472.76979561428146]
mean return 833.476492983
std of return 269.000025832
learning rate: 0.0377964473009
Epoch:  6  avg train loss: 0.248600034695  Reg Loss: 5.63438927027e-06 total time: 45.27238917350769
avg loss validation: 0.24328240074 failed count: 0
learning rate: 0.0353553390593
Epoch:  7  avg train loss: 0.239309164315  Reg Loss: 5.47019030747e-06 total time: 94.04975199699402
avg loss validation: 0.234117103394 failed count: 0
learning rate: 0.0333333333333
Epoch:  8  avg train loss: 0.231091829561  Reg Loss: 5.33742148814e-06 total time: 142.7821180820465
avg loss validation: 0.226680836485 failed count: 0
learning rate: 0.0316227766017
Epoch:  9  avg train loss: 0.224125513039  Reg Loss: 5.2286712195e-06 total time: 191.46141123771667
avg loss validation: 0.220084812672 failed count: 0
learning rate: 0.0301511344578
Epoch:  10  avg train loss: 0.217455742033  Reg Loss: 5.13785436898e-06 total time: 240.21302819252014
avg loss validation: 0.213567968365 failed count: 0
iter 0
iter 1
iter 2
100/1000
iter 3
100/1000
iter 4
100/1000
iter 5
iter 6
100/1000
iter 7
iter 8
iter 9
iter 10
100/1000
200/1000
iter 11
iter 12
100/1000
iter 13
100/1000
iter 14
100/1000
iter 15
iter 16
iter 17
100/1000
iter 18
iter 19
100/1000
returns [516.42757120871011, 358.80162392727601, 754.16421467075668, 724.79238709881236, 656.63975310784019, 555.94839138453403, 621.82594994713531, 583.1860849645501, 456.63184532684369, 482.62870147826578, 1493.9012917715588, 380.80556712957923, 635.91641113581636, 661.04842123395633, 911.6804402269255, 594.16545245176144, 566.94413257201427, 647.48672012514021, 474.2149388468082, 897.37541987008603]
mean return 648.729265924
std of return 240.223970213
learning rate: 0.0288675134595
Epoch:  11  avg train loss: 0.212230775118  Reg Loss: 5.08447974201e-06 total time: 46.046409606933594
avg loss validation: 0.209506028023 failed count: 0
learning rate: 0.0277350098113
Epoch:  12  avg train loss: 0.207057416037  Reg Loss: 5.01333871094e-06 total time: 95.07217025756836
avg loss validation: 0.204031057756 failed count: 0
learning rate: 0.0267261241912
Epoch:  13  avg train loss: 0.201724308107  Reg Loss: 4.94924992729e-06 total time: 143.94679045677185
avg loss validation: 0.199536666736 failed count: 0
learning rate: 0.0258198889747
Epoch:  14  avg train loss: 0.197072601513  Reg Loss: 4.89055826506e-06 total time: 192.78322649002075
avg loss validation: 0.193398760977 failed count: 0
learning rate: 0.025
Epoch:  15  avg train loss: 0.192860875786  Reg Loss: 4.83678370612e-06 total time: 241.69227361679077
avg loss validation: 0.190233050248 failed count: 0
iter 0
iter 1
iter 2
100/1000
iter 3
100/1000
iter 4
100/1000
iter 5
iter 6
iter 7
iter 8
100/1000
iter 9
100/1000
iter 10
iter 11
100/1000
iter 12
iter 13
iter 14
100/1000
iter 15
100/1000
iter 16
100/1000
iter 17
iter 18
100/1000
iter 19
returns [513.52136637146884, 603.17672060909013, 731.83476850904867, 691.45445253305024, 624.62922617622007, 443.27567211523552, 492.87801514393828, 402.84404676602185, 663.35323260946086, 988.66934171688911, 322.50255559363819, 644.93470038494752, 575.1730259635292, 562.83690443653109, 718.89573119678744, 776.94636067206, 633.46096853677739, 333.6579080035483, 804.93655713577721, 581.65296367111978]
mean return 605.531725907
std of return 158.756756818
learning rate: 0.0242535625036
Epoch:  16  avg train loss: 0.189191657157  Reg Loss: 4.76569338453e-06 total time: 45.53456521034241
avg loss validation: 0.186707779521 failed count: 0
learning rate: 0.0235702260396
Epoch:  17  avg train loss: 0.185038135023  Reg Loss: 4.72072694459e-06 total time: 93.17580509185791
avg loss validation: 0.185393932236 failed count: 0
learning rate: 0.0229415733871
Epoch:  18  avg train loss: 0.181900168511  Reg Loss: 4.67846369124e-06 total time: 142.24808931350708
avg loss validation: 0.181239828257 failed count: 0
learning rate: 0.022360679775
Epoch:  19  avg train loss: 0.178478596405  Reg Loss: 4.63899731979e-06 total time: 191.27983283996582
avg loss validation: 0.176538845086 failed count: 0
learning rate: 0.0218217890236
Epoch:  20  avg train loss: 0.17520240659  Reg Loss: 4.59975580415e-06 total time: 240.48588109016418
avg loss validation: 0.174472011305 failed count: 0
iter 0
iter 1
100/1000
iter 2
iter 3
iter 4
100/1000
iter 5
iter 6
iter 7
100/1000
iter 8
iter 9
iter 10
iter 11
iter 12
iter 13
iter 14
iter 15
iter 16
100/1000
iter 17
iter 18
100/1000
iter 19
returns [506.98543942177042, 651.20364596201694, 568.98240943100495, 343.84149296908134, 776.97140072262562, 287.8422834670526, 460.3060103781927, 680.41487046110558, 507.01998777207496, 269.94132571510551, 501.93675713199502, 349.80063764662549, 425.14404574930472, 493.97914442472506, 314.04143138554599, 254.52299227899974, 733.52692915618536, 465.71524463820174, 655.93234729020276, 288.51229760249555]
mean return 476.83103468
std of return 157.787811399
learning rate: 0.0213200716356
Epoch:  21  avg train loss: 0.172469732501  Reg Loss: 4.59178457277e-06 total time: 45.731353998184204
avg loss validation: 0.170874592511 failed count: 0
learning rate: 0.0208514414057
Epoch:  22  avg train loss: 0.169503683768  Reg Loss: 4.55607140899e-06 total time: 96.24083399772644
avg loss validation: 0.166943130665 failed count: 0
learning rate: 0.0204124145232
Epoch:  23  avg train loss: 0.167045661265  Reg Loss: 4.52056899976e-06 total time: 144.55119037628174
avg loss validation: 0.164703867793 failed count: 0
learning rate: 0.02
Epoch:  24  avg train loss: 0.163978089918  Reg Loss: 4.48693302863e-06 total time: 192.6977641582489
avg loss validation: 0.161220958334 failed count: 0
learning rate: 0.0196116135138
Epoch:  25  avg train loss: 0.16143306274  Reg Loss: 4.45303900335e-06 total time: 240.93997168540955
avg loss validation: 0.160467638357 failed count: 0
iter 0
100/1000
iter 1
iter 2
iter 3
iter 4
iter 5
iter 6
iter 7
iter 8
iter 9
iter 10
iter 11
100/1000
iter 12
iter 13
100/1000
iter 14
iter 15
100/1000
iter 16
iter 17
iter 18
iter 19
returns [659.947070509475, 473.42616106762921, 278.48197715644335, 445.26533094781871, 486.0586981974235, 441.65447676161693, 599.40234217411364, 343.83516653212877, 303.74896690455745, 458.56858282327403, 346.95441606095386, 776.48325961469857, 599.97063255413855, 649.89890743180467, 380.14196954383266, 638.01247073583636, 432.45516379537571, 403.85928075952773, 286.49935200980093, 331.57814607123044]
mean return 466.812118583
std of return 139.26978275
learning rate: 0.019245008973
Epoch:  26  avg train loss: 0.15985390686  Reg Loss: 4.40392051676e-06 total time: 44.869457483291626
avg loss validation: 0.156745498457 failed count: 0
learning rate: 0.0188982236505
Epoch:  27  avg train loss: 0.157364926826  Reg Loss: 4.37249283847e-06 total time: 93.09711194038391
avg loss validation: 0.162846602984 failed count: 1
learning rate: 0.0185695338177
Epoch:  28  avg train loss: 0.154836388864  Reg Loss: 4.3406540416e-06 total time: 139.060161113739
avg loss validation: 0.152523272716 failed count: 0
learning rate: 0.0182574185835
Epoch:  29  avg train loss: 0.15376163859  Reg Loss: 4.30981990169e-06 total time: 187.66938304901123
avg loss validation: 0.150733474245 failed count: 0
learning rate: 0.0179605302027
Epoch:  30  avg train loss: 0.150356018891  Reg Loss: 4.27942054532e-06 total time: 235.94233536720276
avg loss validation: 0.149095968772 failed count: 0
iter 0
iter 1
iter 2
iter 3
iter 4
iter 5
iter 6
iter 7
iter 8
iter 9
iter 10
iter 11
iter 12
iter 13
iter 14
iter 15
iter 16
iter 17
iter 18
iter 19
returns [464.12277109023842, 291.85771085938165, 505.66544908322538, 428.7078695664564, 449.91632025236441, 455.31826097210887, 350.09412040266193, 496.32880276246874, 415.28522517417048, 350.62487603897586, 324.69489111018885, 435.00099902949472, 497.1645561820132, 365.68692424660429, 277.48996835906632, 300.72654493730698, 323.83020693883748, 362.79666123108376, 292.23285366039659, 260.15580652317306]
mean return 382.385040921
std of return 78.2040533081
learning rate: 0.0176776695297
Epoch:  31  avg train loss: 0.149093040727  Reg Loss: 4.23629397464e-06 total time: 45.055938482284546
avg loss validation: 0.147979075722 failed count: 0
learning rate: 0.0174077655956
Epoch:  32  avg train loss: 0.147251669064  Reg Loss: 4.20664508718e-06 total time: 93.42724466323853
avg loss validation: 0.148437579795 failed count: 1
learning rate: 0.0171498585143
Epoch:  33  avg train loss: 0.146016085686  Reg Loss: 4.17782787112e-06 total time: 139.10066652297974
avg loss validation: 0.142931590546 failed count: 0
learning rate: 0.0169030850946
Epoch:  34  avg train loss: 0.143958627273  Reg Loss: 4.14879339747e-06 total time: 187.61991119384766
avg loss validation: 0.148191204299 failed count: 1
learning rate: 0.0166666666667
Epoch:  35  avg train loss: 0.141244223487  Reg Loss: 4.11971965641e-06 total time: 233.44750022888184
avg loss validation: 0.144817963332 failed count: 2
iter 0
iter 1
iter 2
iter 3
iter 4
iter 5
100/1000
iter 6
iter 7
iter 8
iter 9
iter 10
iter 11
iter 12
iter 13
100/1000
iter 14
iter 15
iter 16
iter 17
iter 18
iter 19
returns [319.86639041842705, 379.56488305560822, 457.50000364461835, 291.59491541881209, 288.86698003427881, 761.19274313959738, 510.18337103819539, 447.49183730890979, 313.74443324944997, 564.2690933945687, 303.3760658377833, 624.11826093369962, 414.42876266156622, 617.48316202393312, 256.28916056943018, 388.46970927500161, 212.91017165134295, 413.62200520071377, 350.43576800084043, 214.67895632305854]
mean return 406.504333659
std of return 143.619017066
learning rate: 0.0164398987305
Epoch:  36  avg train loss: 0.141160018941  Reg Loss: 4.11918643779e-06 total time: 45.18790078163147
avg loss validation: 0.140972850101 failed count: 0
learning rate: 0.0162221421131
Epoch:  37  avg train loss: 0.139258282235  Reg Loss: 4.09144412097e-06 total time: 93.94695568084717
avg loss validation: 0.140876701939 failed count: 0
learning rate: 0.0160128153805
Epoch:  38  avg train loss: 0.13772054527  Reg Loss: 4.06290873101e-06 total time: 141.13765931129456
avg loss validation: 0.135148145382 failed count: 0
learning rate: 0.0158113883008
Epoch:  39  avg train loss: 0.136178782748  Reg Loss: 4.03579882275e-06 total time: 191.02081060409546
avg loss validation: 0.142284709883 failed count: 1
learning rate: 0.0156173761889
Epoch:  40  avg train loss: 0.135041301307  Reg Loss: 4.00888841293e-06 total time: 239.08419156074524
avg loss validation: 0.131784374471 failed count: 0
iter 0
iter 1
iter 2
iter 3
iter 4
iter 5
iter 6
iter 7
100/1000
iter 8
iter 9
iter 10
100/1000
iter 11
iter 12
iter 13
100/1000
iter 14
iter 15
iter 16
iter 17
iter 18
iter 19
returns [341.10866003905647, 313.89843404531899, 416.42186108742112, 473.73482459803557, 328.68315838480112, 271.15200633593486, 295.82836554607354, 737.50630217319519, 497.56438569888013, 402.8491872097153, 661.34546613275006, 282.91964365667832, 344.97945343963568, 751.04979670727118, 573.04224326017527, 391.13369426714297, 282.24149332506477, 593.16897632712812, 248.64856155366414, 245.7707204044994]
mean return 422.65236171
std of return 157.872195103
learning rate: 0.0154303349962
Epoch:  41  avg train loss: 0.133559181425  Reg Loss: 3.96883141749e-06 total time: 46.433708906173706
avg loss validation: 0.134028206924 failed count: 0
learning rate: 0.0152498570333
Epoch:  42  avg train loss: 0.132949287896  Reg Loss: 3.94396599345e-06 total time: 96.43728041648865
avg loss validation: 0.131402455627 failed count: 0
learning rate: 0.0150755672289
Epoch:  43  avg train loss: 0.131031977338  Reg Loss: 3.91906029887e-06 total time: 145.18159103393555
avg loss validation: 0.132692699679 failed count: 1
learning rate: 0.01490711985
Epoch:  44  avg train loss: 0.130274551336  Reg Loss: 3.89410367127e-06 total time: 191.18478560447693
avg loss validation: 0.127731270136 failed count: 0
learning rate: 0.0147441956155
Epoch:  45  avg train loss: 0.128472648973  Reg Loss: 3.87015768814e-06 total time: 239.93791890144348
avg loss validation: 0.127121757496 failed count: 0
iter 0
iter 1
iter 2
iter 3
iter 4
iter 5
iter 6
iter 7
iter 8
iter 9
iter 10
iter 11
iter 12
iter 13
iter 14
iter 15
iter 16
iter 17
iter 18
iter 19
returns [316.70596341560764, 354.50057806803028, 307.51526596595312, 427.55495124453802, 603.14640098943914, 351.36315690597382, 277.22192733734818, 443.61254580337726, 275.88875861814506, 284.27348580689062, 273.29186621304774, 330.06593620384632, 256.61450583194573, 293.50428414630528, 346.96864000706205, 364.86378072500122, 416.29898261456185, 576.15181562964688, 252.62125912994466, 352.11747852134329]
mean return 355.214079159
std of return 94.8192091723
learning rate: 0.0145864991498
